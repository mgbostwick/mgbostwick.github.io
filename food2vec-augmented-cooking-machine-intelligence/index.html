

<!DOCTYPE html>
<meta charset="utf-8">
<title>food2vec - Augmented cooking with machine intelligence &#8211; Jaan Altosaar</title>
<meta name="description" content="Building a recommendation system for food & exploring the world's cuisines.">
<meta name="keywords" content="cuisine, food, word2vec, embedding, machine learning, artificial intelligence, statistics, machine intelligence, AI">



<head>

<!-- Twitter Cards -->

<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="/images/mise-en-place.jpg">

<meta name="twitter:title" content="food2vec - Augmented cooking with machine intelligence">
<meta name="twitter:description" content="Building a recommendation system for food & exploring the world's cuisines.">
<meta name="twitter:creator" content="@thejaan">

<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:title" content="food2vec - Augmented cooking with machine intelligence &#8211; Jaan Altosaar">
<meta property="og:description" content="Building a recommendation system for food & exploring the world's cuisines.">
<meta property="og:url" content="/food2vec-augmented-cooking-machine-intelligence/">
<meta property="og:site_name" content="Jaan Altosaar">
<meta property="og:image" content="/images/food2vec-icon.png">

<!-- Google authorship -->
<a rel="author" href="https://www.google.com/+JaanAltosaar"></a>

<!-- Google & Bing Verification -->
<meta name="google-site-verification" content="LTwvMh1s7cdUyjaHqQYsBg-I4ZPy0wm_TKUoU0IT5HU#LTwvMh1s7cdUyjaHqQYsBg-I4ZPy0wm_TKUoU0IT5HU">
<meta name="msvalidate.01" content="B3B21CDB59D1FC75DFE9B0D0CC329C8C">
</head>
<link rel="canonical" href="/food2vec-augmented-cooking-machine-intelligence/">
<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Subscribe to the Jaan Feed">



<!-- old feed: <link href="/feed.xml" target="_blank" type="application/atom+xml" rel="alternate" title="Subscribe to the Jaan RSS Feed"> -->

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- Type -->
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Crimson+Text:400,400italic,700,700italic" rel='stylesheet' type='text/css' />
<link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700" rel='stylesheet' type='text/css'>
<!-- <link rel="stylesheet" href="/assets/css/entypo.css" media="all"> -->
<script async src="https://use.fontawesome.com/0a87e83674.js"></script>
<!-- In order to use Calendas Plus, you must first purchase it. Then, create a font-face package using FontSquirrel.
<link rel='stylesheet' href='/assets/cal.css' media='all' />
-->

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/i.css">

<!-- Fresh Squeezed jQuery -->
<script async src="https://ajax.googleapis.com/ajax/libs/jquery/1/jquery.min.js" type="text/javascript"></script>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js" type="text/javascript"></script>
<!-- <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> -->
<script async src="/assets/js/auto-render.min.js"></script>

<meta http-equiv="cleartype" content="on">

<script async src="/assets/js/main.js"></script>

<!-- Load Modernizr -->
<script async src="/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>


<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="/favicon.png">

<div id="bump">
  <body class="">
    <header class="site-header darken">
      <div class="wrap">
        <hgroup>
          <h1><a href="/"><span id="jaan" class="unhilighted">Jaan</span>&nbsp;<span id="altosaar" class="hilighted">Altosaar</span></a></h1>
        </hgroup>
        <a href="#nav" class="menu"><span class='icons'>☰</span></a>
        <nav role="navigation">
          <ul>
            <!--<li>
              <a href="/" title="Jaan">Home</a>
            </li>-->
            <ul>
            
            
                
                
                <li><a href="/about" >About</a></li>
            
                
                
                <li><a href="/articles" >Articles</a></li>
            
                
                
                <li><a href="/projects" >Projects</a></li>
            
                
                
                <li><a href="/publications" >Papers</a></li>
            
                
                
                <li><a href="/talks" >Talks</a></li>
            

          </ul>
        </nav>
      </div>
    </header>


<section class="article">

  <div class="overlay overlay-lighter"></div>
  <div class="featured-image" style="background-image: url(/images/mise-en-place.jpg)"></div>


      <article class="wrap post">
        <header class="post-header">
          <hgroup>
            <h1><a href="/food2vec-augmented-cooking-machine-intelligence/">food2vec - Augmented cooking with machine intelligence</a></h1>
            <br>
            <!-- <p class="date">Published on <span class="date">Jan 22, 2017</span>&nbsp;|&nbsp;By <span itemprop="name" class="fn"><a href="/about" title="About Jaan Altosaar" itemprop="url">Jaan Altosaar</a></span> | <a href="/food2vec-augmented-cooking-machine-intelligence/">Permalink</a></p> -->

            <p class="intro">Building a recommendation system for food & exploring the world's cuisines.</p>
          </hgroup>
        </header>

        <p><strong>TL;DR: Check out the <a href="https://altosaar.github.io/food2vec/">tools demo</a> to explore food analogies and recommendations, or scroll down for an interactive map of a hundred thousand recipes from around the world.</strong></p>

<p>I haven’t eaten in five days. I dream of food. I study food. Deep in ketosis, my body has adapted to consume itself: I am food. There is no better time to dig into modeling grub.</p>

<p>Machine intelligence has changed your life, from how you listen to music through Discover Weekly playlists, consume news through Facebook, or talk to your hand computer’s friendly digital assistant. But why hasn’t it changed how we eat? Can we modify the ingredients of language processing algorithms to get insights about food? If you tell me what you want to eat, can I recommend complementary foods, much like Spotify recommends complementary songs?</p>

<p>Word embeddings are a useful technique for analyzing discrete data. Say we use <script type="math/tex">170,000</script> words from the Oxford English dictionary. We can represent each word (such as “food”) as a vector as follows: a list of <script type="math/tex">169,999</script> zeros, with a single <script type="math/tex">1</script> at the location of the word in the vocabulary. In our case, “food” may be at location <script type="math/tex">29,163</script> near other words beginning with the letter f. Then the vector for “food” would look like:</p>

<script type="math/tex; mode=display">[0, 0, 0, ..., 0, 0, 1, 0, 0, ..., 0].</script>

<p>However, this is inadequate for comparing words. To compare documents and get useful insights from our data, we need to aggregate over <script type="math/tex">170,000</script> dimensions for each word, which takes far too long. Can we do better?</p>

<p>Embeddings let us reduce the dimensionality of the problem, and give us a powerful representation of language. We can build a model of language where we assign a hundred random numbers to each word. To train the model, we use these hundred numbers of each word to predict their context. The “context” of a word consists of its surrounding words. This is the main idea: the context means that words that occur in similar contexts should have similar meanings. We tweak tweak the numbers assigned to a word to make them better at predicting words in the context. Initially, the random numbers assigned to a word will be bad at predicting words in the context. But gradually, through this process of tweaking the model’s predictions of surrounding words, we get a hundred numbers that are far from random. The hundred numbers representing each word will capture part of its meaning: similar words will cluster together because they occur in each other’s contexts, and words with different meanings are pushed far apart (out-of-context). By representing each word as an embedding in <script type="math/tex">100</script> dimensions, we have reduced the dimensionality more than a thousandfold from <script type="math/tex">170,000</script> and gained a better representation of language.</p>

<p>For modeling food, we have a collection of recipes. We can define the context of an ingredient in a recipe to be the rest of the foods in the recipe. This demonstrates the flexibility of embeddings: by making a small change in the definition of the context, we can now apply it to a totally different kind of data.</p>

<h3 id="food-similarity-map">Food similarity map</h3>

<p>After training the embedding algorithm on a collection of <script type="math/tex">95, 896</script> recipes, we get <script type="math/tex">100</script>-dimensional embeddings for each food. Humans can’t visualize high dimensions, so we use an approximation technique to visualize similarity between the foods in two dimensions.</p>

<p>Here is a similarity map of the <script type="math/tex">2,087</script> ingredients used in the recipes. Hover over a point to see which food it represents:</p>

<iframe width="100%" height="800" frameborder="0" scrolling="no" src="/files/food2vec_food_embeddings_tsne.html"></iframe>

<p>The map of foods is reasonable. Ingredients from Asia cluster together, as do ingredients used in European and North American cooking.</p>

<h3 id="recipe-embedding-map">Recipe embedding map</h3>

<p>We can generate an embedding for a recipe by taking the average of its ingredients’ embeddings. Here is a map of <script type="math/tex">95, 896</script> recipes from around the world. Hover over a point to see the recipe, and click on the cuisine legend on the right to show or hide certain regions:</p>

<p><em>IMPORTANT: you are about to download 15MB of data.</em> Click <a href="/files/food2vec_recipe_embeddings_tsne.html">here</a> to access the map, zoom in, and discover new flavors. Is this the fastest way to browse 100k recipes by similarity?</p>

<p>Interesting patterns emerge. Asian recipes cluster together, as do Southern European recipes. Northern European and American foods are all over the place, maybe because of transmission of recipes due to migration, or over-representation in the data.</p>

<h3 id="food-similarity-tool">Food similarity tool</h3>

<p>Access the tool at <a href="https://altosaar.github.io/food2vec/#food-similarity-tool">this link</a>. We can calculate food similarity by looking at which food is closest in the high dimensional space in the embeddings.</p>

<p>These mostly make sense - foods are closest to other foods they appear with in recipes:</p>
<ul>
  <li>Cheese is closest to macaroni</li>
  <li>Sesame oil is closest to egg noodle</li>
  <li>Milk is closest to nutmeg</li>
  <li>Olive oil is closest to parmesan cheese</li>
</ul>

<h3 id="food-analogy-tool">Food analogy tool</h3>

<p>Access the tool <a href="https://altosaar.github.io/food2vec/#food-analogy-tool">here</a>. Food analogies, like word analogies, are calculated with vector arithmetic. For the analogy “Food A is to food B, as food C is to food D”, the goal is to predict a reasonable food D. We can do this by subtracting food B from food A, then adding food C. For example, calculating <script type="math/tex">(bacon - egg) + orangejuice</script> in embedding space will yield an embedding. The closest embedding to this is <script type="math/tex">coffee</script> in our model of food. The classic example from word embeddings is <script type="math/tex">(king - man) + woman = queen</script>. Is this intuitive? King is to man as woman is to queen makes sense in natural language, but food analogies are less clear. With practice, we may be able to train our taste detectors and devise hypotheses to test in the realm of food. I also included cuisine embeddings by representing them as the average of their recipes’ embeddings.</p>

<p>Some of these are more plausible than others:</p>
<ul>
  <li>Egg is to bacon as orange juice is to coffee.</li>
  <li>Bread is to butter as roast beef is to sage.</li>
  <li>Smoked salmon is to dill as lamb is to asparagus.</li>
  <li>South Asian is to rice as Southern European is to thyme.</li>
  <li>Rice is to sesame seed as macaroni is to pimento.</li>
  <li>Roasted beef is to green bell pepper as pork sausage is to fenugreek.</li>
</ul>

<h3 id="recipe-recommendation-tool">Recipe recommendation tool</h3>

<p>Access the tool <a href="https://altosaar.github.io/food2vec/#recipe-recommendation-tool">here</a>. We can use our model of food as a recommendation system for cooks. By taking the average embedding for a set of foods, we can look up foods with the closest embeddings.</p>

<p>For example, I am a lifelong aficionado of peanut butter jam sandwiches. I entered my usual favorite: white bread, butter, peanut butter, honey. The top recommendation was: strawberry. I’ve never tried that, and it’s pretty good! I happily broke my fast with it. For the recipe of lamb, cumin, tomato, the top recommendation is raisin - also reasonable and interesting. Other recommendations are a bit wackier, so best of luck.</p>

<p>If you end up adding an ingredient to your food based on these tools, I’d love to hear how it tasted: ping me on <a href="https://twitter.com/thejaan">Twitter</a> or <a href="mailto:altosaar@princeton.edu">email</a>!</p>

<h3 id="whats-next">What’s next?</h3>

<ul>
  <li>Figuring out the right user interface to explore these models. The code for the plots and recommendation tools is on <a href="https://github.com/altosaar/food2vec">github</a>. It would be great to make these mobile-friendly and test other ways of presenting recommendations from the model to users.</li>
  <li>word2vec is not the best model for this. Multi-class regression should work well, and I added a working <a href="https://github.com/altosaar/food2vec/blob/master/src/food2vec.py">demo of this</a> to the repo. This is a rare case where the vocabulary size (number of ingredients) is very small, so we can fit both models and compare them. This could reveal idiosyncrasies in the non-contrastive estimation loss used in word2vec and provides an interesting testbed.</li>
  <li>Scaling up the data:  Do you have a larger dataset of recipes, or do you know how to scrape one? I’d love to check it out. This would also fix bias in the data as the majority of the recipes are currently North American.</li>
  <li>Testing out recipe analogies combined with food analogies: this may be more intuitive for us humans. For example, “pancakes are to maple syrup, as an omelette is to cheese” could be easier to think about than analogies with individual ingredients.</li>
</ul>

<h3 id="resources">Resources</h3>

<ul>
  <li>This NYT piece, <a href="https://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html">The Great AI Awakening</a>, does a much better job at describing embeddings than I can</li>
  <li>Wesley has a neat paper on a similar approach: <a href="https://arxiv.org/abs/1612.00388">diet2vec</a></li>
  <li>Sanjeev Arora’s <a href="http://www.offconvex.org/2016/02/14/word-embeddings-2/">research</a> has good explanations for the analogy properties of embeddings</li>
  <li>The <a href="https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm">t-SNE algorithm</a> for visualizing high-dimensional embeddings</li>
  <li>The original <a href="http://www.nature.com/articles/srep00196">Nature Scientific Report</a> with the data</li>
  <li>Dave taught a fantastic <a href="http://www.cs.columbia.edu/~blei/seminar/2016_discrete_data/index.html">class</a> that helped me understand embeddings</li>
  <li>Maja’s paper on <a href="https://arxiv.org/abs/1608.00778">exponential family embeddings</a> generalizes word2vec to other distributions that would be neat to try on this data (word2vec can be interpreted as a Bernoulli embedding model with biased gradients)</li>
  <li>There are a few other versions of food2vec floating around, like <a href="https://automateddeveloper.blogspot.com/2016/10/unsupervised-learning-in-scala-using.html">Rob Hinds’</a></li>
</ul>

<p><em>Thanks to <a href="http://www.cs.columbia.edu/~blei/">Dave</a> for the idea, <a href="http://sociology.columbia.edu/node/66">Peter Bearman</a> for presenting his work to our group, <a href="https://www.flickr.com/photos/mealmakeovermoms/">MealMakeOverMoms</a> for the mise photo, <a href="http://anthony.ai/">Anthony</a> for open-sourcing the embedding browser on which ours is based,  and <a href="https://plot.ly/">Plotly</a> for open-sourcing their fantastic plotting library.</em></p>

<p>Feel free to ping me on <a href="https://twitter.com/thejaan">Twitter</a> or <a href="mailto:altosaar@princeton.edu">email</a> with feedback or ideas!</p>

<p>Discussion on <a href="https://news.ycombinator.com/item?id=13472721">Hacker News</a> and <a href="https://www.reddit.com/r/MachineLearning/comments/5px9uz/p_food_visualization_and_recommendation_engine_in/">Reddit</a>. Also see <a href="https://github.com/altosaar/food2vec/blob/master/doc/food2vec-nytimes-talk.pdf">slides</a> from a talk at the New York Times on this project.</p>


      <!--<div class="sharet"><a class="share" href="https://twitter.com/intent/tweet?text=https://jaan.io/food2vec-augmented-cooking-machine-intelligence/+%40thejaan" target ="_blank" data-dnt="true"><i class="icon-twitter"></i></a>&nbsp;&nbsp;&nbsp;<a class="share" href="https://www.facebook.com/sharer/sharer.php?u=https://jaan.io/food2vec-augmented-cooking-machine-intelligence/" target="_blank"><i class="icon-facebook"></i></a>&nbsp;&nbsp;&nbsp;</div>-->

      <!--<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="//platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>-->

      

      </article>
    </section>
</div>
<script>
    var scripts = document.getElementsByTagName("script");
    for (var i = 0; i < scripts.length; i++) {
      /* TODO: keep going after an individual parse error. */
      var script = scripts[i];
      if (script.type.match(/^math\/tex/)) {
        var text = script.text === "" ? script.innerHTML : script.text;
        var options = script.type.match(/mode\s*=\s*display/) ?
                      {displayMode: true} : {};
        script.insertAdjacentHTML("beforebegin",
                                  katex.renderToString(text, options));
      }
    }
    document.body.className += " math_finished";
</script>

<div class="push"></div>
  <footert>
  <footer>
    <!--<aside class="wrap">
      <ol class="prev-posts">
        <p class="list-title"><a href="/">Recent Articles</a></p>
        
            <li>
              <span class="recent-title"><a href="/food2vec-augmented-cooking-machine-intelligence/" title="food2vec - Augmented cooking with machine intelligence">food2vec - Augmented cookin...</span>
              <span class="date">Jan 22, 2017</a></span>
            </li>
        
            <li>
              <span class="recent-title"><a href="/operator-variational-inference/" title="Operator Variational Inference">Operator Variational Inference</span>
              <span class="date">Oct 31, 2016</a></span>
            </li>
        
            <li>
              <span class="recent-title"><a href="/factorization-item-embedding/" title="Word embedding models for recommendation">Word embedding models for r...</span>
              <span class="date">Aug 05, 2016</a></span>
            </li>
        
      </ol>

      <div class="social">
        <ul>
            <li><a href="/about"><span class="foot-link">About</span></a></li>

            <li><a href="/articles"><span class="foot-link">Articles</span></a></li>

            <li><a id="mail" href="https://jaan.io/feed.xml"><span class="foot-link">Subscribe</span></a></li>



            
            <li><a id="twit" href="http://twitter.com/thejaan" target="_blank"><span class="foot-link">@thejaan</span></a></li>
            


            
        </ul>
    </div>
    </aside>-->
    <!-- <small>&copy; 2017 <a href="mailto:jaan@jaan.io"> Jaan Altosaar </a></small> -->
    <small><a href="https://twitter.com/thejaan" target="_blank"><i class="fa fa-twitter"></i></a></small>
  </footer>
</footert>

  <!-- If they're out, get some from the cellar -->
  <script>window.jQuery || document.write('<script src="/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
  <script src="/assets/js/retina.min.js"></script>

  <!-- Custom JS -->
  <script async src="/assets/js/scripts.js"></script>


  
  <!-- Asynchronous Google Analytics snippet -->
  <script>
    var _gaq = _gaq || [];
    var pluginUrl =
    '//www.google-analytics.com/plugins/ga/inpage_linkid.js';
    _gaq.push(['_require', 'inpage_linkid', pluginUrl]);
    _gaq.push(['_setAccount', 'UA-34129661-2']);
    _gaq.push(['_trackPageview']);

  setTimeout(function() {
    window.onscroll = function() {
      window.onscroll = null; // Only track the event once
      _gaq.push(['_trackEvent', 'scroll', 'read']);
    }
  }, 5000);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://' : 'http://') + 'stats.g.doubleclick.net/dc.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  

  </body>
</html>
